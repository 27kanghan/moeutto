{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5a3b7e88e48ef51b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eeadaa086d003e1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -U fashion-clip"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a50b6c656a5e6cd1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append(\"fashion-clip/\")\n",
    "from fashion_clip.fashion_clip import FashionCLIP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T02:20:34.289618300Z",
     "start_time": "2023-11-15T02:20:27.904195500Z"
    }
   },
   "id": "ae59135cfaefd8af"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 테스트 용 이미지 \n",
    "img = Image.open('data_for_fashion_clip/108775044.jpg')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T03:27:12.666332700Z",
     "start_time": "2023-11-15T03:27:12.635545500Z"
    }
   },
   "id": "bfa0a2a8c1a8834b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fclip = FashionCLIP('fashion-clip')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "544b07d54bacf1d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 한번에 32개 임베딩 \n",
    "images = [] # 예제에서는 파일로 불러옴\n",
    "image_embeddings = fclip.encode_images(images, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd7c4b86230cf703"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 텍스트 임베딩 예제\n",
    "text_embedding = fclip.encode_text([\"a pair of pink shorts\"], 32)[0]\n",
    "\n",
    "id_of_matched_object = np.argmax(text_embedding.dot(image_embeddings.T))\n",
    "\n",
    "found_object = subset[\"article_id\"].iloc[id_of_matched_object].tolist()\n",
    "\n",
    "fixed_height = 224\n",
    "\n",
    "image = Image.open(f\"data_for_fashion_clip/{found_object}.jpg\")\n",
    "height_percent = (fixed_height / float(image.size[1]))\n",
    "width_size = int((float(image.size[0]) * float(height_percent)))\n",
    "image = image.resize((width_size, fixed_height), Image.NEAREST)\n",
    "\n",
    "image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6dde400b53d7b5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 코사인 유사도 대신 내적을 사용하기 위한 표준화(normalize) \n",
    "image_embeddings = image_embeddings/np.linalg.norm(image_embeddings, ord=2, axis=-1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9290061fe35625c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
