{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-14T08:31:38.051290700Z",
     "start_time": "2023-11-14T08:30:23.863861400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fashion-clip\n",
      "  Downloading fashion_clip-0.2.2-py3-none-any.whl (15 kB)\n",
      "Collecting annoy>=1.17.0\n",
      "  Using cached annoy-1.17.3.tar.gz (647 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas>=1.3.5 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from fashion-clip) (2.1.3)\n",
      "Collecting pyarrow>=7.0.0\n",
      "  Downloading pyarrow-14.0.1-cp310-cp310-win_amd64.whl (24.6 MB)\n",
      "     ---------------------------------------- 24.6/24.6 MB 3.0 MB/s eta 0:00:00\n",
      "Collecting matplotlib>=3.5.1\n",
      "  Downloading matplotlib-3.8.1-cp310-cp310-win_amd64.whl (7.6 MB)\n",
      "     ---------------------------------------- 7.6/7.6 MB 3.7 MB/s eta 0:00:00\n",
      "Collecting transformers>=4.26.1\n",
      "  Using cached transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "Collecting validators\n",
      "  Using cached validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Collecting boto3>=1.10.50\n",
      "  Downloading boto3-1.28.85-py3-none-any.whl (135 kB)\n",
      "     -------------------------------------- 135.8/135.8 kB 4.1 MB/s eta 0:00:00\n",
      "Collecting torch>=1.11.0\n",
      "  Using cached torch-2.1.0-cp310-cp310-win_amd64.whl (192.3 MB)\n",
      "Collecting datasets>=2.10.0\n",
      "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "     -------------------------------------- 493.7/493.7 kB 3.9 MB/s eta 0:00:00\n",
      "Collecting ipyplot>=1.1.1\n",
      "  Using cached ipyplot-1.1.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.19.2 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from fashion-clip) (1.0.0)\n",
      "Collecting appdirs>=1.4.4\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting s3transfer<0.8.0,>=0.7.0\n",
      "  Using cached s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting botocore<1.32.0,>=1.31.85\n",
      "  Downloading botocore-1.31.85-py3-none-any.whl (11.3 MB)\n",
      "     ---------------------------------------- 11.3/11.3 MB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from datasets>=2.10.0->fashion-clip) (2023.10.0)\n",
      "Requirement already satisfied: packaging in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from datasets>=2.10.0->fashion-clip) (23.2)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.6-cp310-cp310-win_amd64.whl (325 kB)\n",
      "     -------------------------------------- 325.2/325.2 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from datasets>=2.10.0->fashion-clip) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from datasets>=2.10.0->fashion-clip) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from datasets>=2.10.0->fashion-clip) (1.26.2)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "     -------------------------------------- 134.8/134.8 kB 8.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from datasets>=2.10.0->fashion-clip) (0.17.3)\n",
      "Collecting dill<0.3.8,>=0.3.0\n",
      "  Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from datasets>=2.10.0->fashion-clip) (2.31.0)\n",
      "Requirement already satisfied: pillow in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from ipyplot>=1.1.1->fashion-clip) (10.1.0)\n",
      "Collecting shortuuid\n",
      "  Using cached shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: IPython in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from ipyplot>=1.1.1->fashion-clip) (8.17.2)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-win_amd64.whl (186 kB)\n",
      "     -------------------------------------- 186.7/186.7 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-win_amd64.whl (56 kB)\n",
      "     ---------------------------------------- 56.1/56.1 kB ? eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.44.0-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 5.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from matplotlib>=3.5.1->fashion-clip) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from pandas>=1.3.5->fashion-clip) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from pandas>=1.3.5->fashion-clip) (2023.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from torch>=1.11.0->fashion-clip) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from torch>=1.11.0->fashion-clip) (1.12)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: filelock in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from torch>=1.11.0->fashion-clip) (3.13.1)\n",
      "Requirement already satisfied: jinja2 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from torch>=1.11.0->fashion-clip) (3.1.2)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.0-cp310-none-win_amd64.whl (277 kB)\n",
      "     ------------------------------------- 277.4/277.4 kB 16.7 MB/s eta 0:00:00\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.10.3-cp310-cp310-win_amd64.whl (269 kB)\n",
      "     -------------------------------------- 269.6/269.6 kB 8.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from transformers>=4.26.1->fashion-clip) (0.14.1)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from botocore<1.32.0,>=1.31.85->boto3>=1.10.50->fashion-clip) (1.26.18)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-win_amd64.whl (44 kB)\n",
      "     ---------------------------------------- 44.4/44.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from aiohttp->datasets>=2.10.0->fashion-clip) (3.3.2)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from aiohttp->datasets>=2.10.0->fashion-clip) (23.1.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp310-cp310-win_amd64.whl (61 kB)\n",
      "     ---------------------------------------- 61.0/61.0 kB ? eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.5.1->fashion-clip) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from requests>=2.19.0->datasets>=2.10.0->fashion-clip) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from requests>=2.19.0->datasets>=2.10.0->fashion-clip) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from tqdm>=4.62.1->datasets>=2.10.0->fashion-clip) (0.4.6)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from IPython->ipyplot>=1.1.1->fashion-clip) (5.13.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from IPython->ipyplot>=1.1.1->fashion-clip) (1.1.3)\n",
      "Requirement already satisfied: stack-data in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from IPython->ipyplot>=1.1.1->fashion-clip) (0.6.3)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from IPython->ipyplot>=1.1.1->fashion-clip) (0.19.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from IPython->ipyplot>=1.1.1->fashion-clip) (2.16.1)\n",
      "Requirement already satisfied: decorator in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from IPython->ipyplot>=1.1.1->fashion-clip) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from IPython->ipyplot>=1.1.1->fashion-clip) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from IPython->ipyplot>=1.1.1->fashion-clip) (3.0.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->fashion-clip) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from sympy->torch>=1.11.0->fashion-clip) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from jedi>=0.16->IPython->ipyplot>=1.1.1->fashion-clip) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->IPython->ipyplot>=1.1.1->fashion-clip) (0.2.10)\n",
      "Requirement already satisfied: pure-eval in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from stack-data->IPython->ipyplot>=1.1.1->fashion-clip) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from stack-data->IPython->ipyplot>=1.1.1->fashion-clip) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\dev\\s09p31a604\\crawling\\img-crawling\\venv\\lib\\site-packages (from stack-data->IPython->ipyplot>=1.1.1->fashion-clip) (2.4.1)\n",
      "Building wheels for collected packages: annoy\n",
      "  Building wheel for annoy (setup.py): started\n",
      "  Building wheel for annoy (setup.py): finished with status 'done'\n",
      "  Created wheel for annoy: filename=annoy-1.17.3-cp310-cp310-win_amd64.whl size=52301 sha256=3e63a645f02fcbd5e09b4cdfe750dc10773850f0cb5a97f2c42a0b39d4a1f835\n",
      "  Stored in directory: c:\\users\\ssafy\\appdata\\local\\pip\\cache\\wheels\\6a\\61\\46\\4c4972710c46d8ec375b9241bfa160b7a13f765dd9eb1220df\n",
      "Successfully built annoy\n",
      "Installing collected packages: appdirs, annoy, xxhash, validators, shortuuid, safetensors, regex, pyparsing, pyarrow, networkx, multidict, kiwisolver, jmespath, frozenlist, fonttools, dill, cycler, contourpy, async-timeout, yarl, torch, multiprocess, matplotlib, botocore, aiosignal, s3transfer, aiohttp, transformers, ipyplot, boto3, datasets, fashion-clip\n",
      "Successfully installed aiohttp-3.8.6 aiosignal-1.3.1 annoy-1.17.3 appdirs-1.4.4 async-timeout-4.0.3 boto3-1.28.85 botocore-1.31.85 contourpy-1.2.0 cycler-0.12.1 datasets-2.14.6 dill-0.3.7 fashion-clip-0.2.2 fonttools-4.44.0 frozenlist-1.4.0 ipyplot-1.1.1 jmespath-1.0.1 kiwisolver-1.4.5 matplotlib-3.8.1 multidict-6.0.4 multiprocess-0.70.15 networkx-3.2.1 pyarrow-14.0.1 pyparsing-3.1.1 regex-2023.10.3 s3transfer-0.7.0 safetensors-0.4.0 shortuuid-1.0.11 torch-2.1.0 transformers-4.35.0 validators-0.22.0 xxhash-3.4.1 yarl-1.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U fashion-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m classification_report\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#sys.path.append(\"fashion-clip/\")\n",
    "from fashion_clip.fashion_clip import FashionCLIP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T08:32:14.269465900Z",
     "start_time": "2023-11-14T08:32:00.333282500Z"
    }
   },
   "id": "b2759c9ad8755a18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fclip = FashionCLIP('fashion-clip')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc04f236aa1ab0b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "articles = pd.read_csv(\"data_for_fashion_clip/articles.csv\")\n",
    "\n",
    "# drop items that have the same description\n",
    "subset = articles.drop_duplicates(\"detail_desc\").copy()\n",
    "\n",
    "# remove items of unkown category\n",
    "subset = subset[~subset[\"product_group_name\"].isin([\"Unknown\"])]\n",
    "\n",
    "# FashionCLIP has a limit of 77 tokens, let's play it safe and drop things with more than 40 tokens\n",
    "subset = subset[subset[\"detail_desc\"].apply(lambda x : 4 < len(str(x).split()) < 40)]\n",
    "\n",
    "# We also drop products types that do not occur very frequently in this subset of data\n",
    "most_frequent_product_types = [k for k, v in dict(Counter(subset[\"product_type_name\"].tolist())).items() if v > 10]\n",
    "subset = subset[subset[\"product_type_name\"].isin(most_frequent_product_types)]\n",
    "\n",
    "# lots of data here, but we will just use only descriptions and a couple of other columns\n",
    "subset.head(3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41293a5e7214e05b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subset.to_csv(\"subset_data.csv\", index=False)\n",
    "f\"There are {len(subset)} elements in the dataset\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1329dc4ea6b129cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images = [\"data_for_fashion_clip/\" + str(k) + \".jpg\" for k in subset[\"article_id\"].tolist()]\n",
    "texts = subset[\"detail_desc\"].tolist()\n",
    "\n",
    "# we create image embeddings and text embeddings\n",
    "image_embeddings = fclip.encode_images(images, batch_size=32)\n",
    "text_embeddings = fclip.encode_text(texts, batch_size=32)\n",
    "\n",
    "# we normalize the embeddings to unit norm (so that we can use dot product instead of cosine similarity to do comparisons)\n",
    "image_embeddings = image_embeddings/np.linalg.norm(image_embeddings, ord=2, axis=-1, keepdims=True)\n",
    "text_embeddings = text_embeddings/np.linalg.norm(text_embeddings, ord=2, axis=-1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9f48575ce8d9742"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d3d89a843bbab6c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision = 0\n",
    "\n",
    "# we could batch this operation to make it faster\n",
    "for index, t in enumerate(text_embeddings):\n",
    "    arr = t.dot(image_embeddings.T)\n",
    "\n",
    "    best = arr.argsort()[-5:][::-1]\n",
    "\n",
    "    if index in best:\n",
    "        precision +=1\n",
    "\n",
    "round(precision/len(text_embeddings), 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a73c974b0845fa22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_embedding = fclip.encode_text([\"a pair of pink shorts\"], 32)[0]\n",
    "\n",
    "id_of_matched_object = np.argmax(text_embedding.dot(image_embeddings.T))\n",
    "found_object = subset[\"article_id\"].iloc[id_of_matched_object].tolist()\n",
    "\n",
    "fixed_height = 224\n",
    "\n",
    "image = Image.open(f\"data_for_fashion_clip/{found_object}.jpg\")\n",
    "height_percent = (fixed_height / float(image.size[1]))\n",
    "width_size = int((float(image.size[0]) * float(height_percent)))\n",
    "image = image.resize((width_size, fixed_height), Image.NEAREST)\n",
    "\n",
    "image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20be3c13dabe6907"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_embedding = fclip.encode_text([\"a house\"], 32)[0]\n",
    "\n",
    "id_of_matched_object = np.argmax(text_embedding.dot(image_embeddings.T))\n",
    "found_object = subset[\"article_id\"].iloc[id_of_matched_object].tolist()\n",
    "\n",
    "fixed_height = 224\n",
    "\n",
    "image = Image.open(f\"data_for_fashion_clip/{found_object}.jpg\")\n",
    "height_percent = (fixed_height / float(image.size[1]))\n",
    "width_size = int((float(image.size[0]) * float(height_percent)))\n",
    "image = image.resize((width_size, fixed_height), Image.NEAREST)\n",
    "\n",
    "image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70441081a2ad34ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = subset[\"product_type_name\"].unique()\n",
    "print(f\"These are our labels: {labels}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10884cfcb306b778"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_prompt = [f\"a photo of a {k}\" for k in labels]\n",
    "\n",
    "label_embeddings = fclip.encode_text(labels_prompt, batch_size=32)\n",
    "label_embeddings = label_embeddings/np.linalg.norm(label_embeddings, ord=2, axis=-1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd2cb2bc7e788793"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e037202c0f84b042"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's compute the class distribution for all images\n",
    "predicted_classes_distribution = label_embeddings.dot(image_embeddings.T)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79d7a316ea6261fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's take the best label (the most similar to the image)\n",
    "predicted = [labels[k] for k in np.argmax(predicted_classes_distribution, axis=0)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7532f47b417dad19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(classification_report(subset[\"product_type_name\"], predicted))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a47ec345a3b30534"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classes = subset[\"product_group_name\"].tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_embeddings, classes, test_size=0.20, random_state=32, stratify=classes)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6871d751dc387c61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, class_weight=\"balanced\").fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7325c65107d194b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(classification_report(y_test, clf.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a8f60a705718de9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
